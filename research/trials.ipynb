{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348b2522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nihar\\\\OneDrive\\\\Desktop\\\\debatemodel\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97da5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd94282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nihar\\OneDrive\\Desktop\\debatemodel\n",
      "['data\\\\21 Lessons for the 21st Century by Yuval Noah Harari.pdf', 'data\\\\Argumentation and Debates.pdf', 'data\\\\Factfulness by Hans Rosling.pdf', 'data\\\\Thank You for Arguing_ What Aristotle, Lincoln, and Homer Simpson Can Teach Us .. by Jay Heinrichs.pdf', 'data\\\\The_Art_of_Public_Speaking.pdf', 'data\\\\Thinking, Fast and Slow by Daniel Kahneman.pdf']\n",
      "['data\\\\1.csv', 'data\\\\2.csv', 'data\\\\3.csv', 'data\\\\4.csv', 'data\\\\5.csv', 'data\\\\6.csv', 'data\\\\7.csv', 'data\\\\8.csv', 'data\\\\9.csv']\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "print(os.getcwd())  # current working dir\n",
    "print(glob.glob(\"data/*.pdf\"))  # list PDFs inside data/\n",
    "print(glob.glob(\"data/*.csv\"))  # list PDFs inside data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "902f82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc35b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n"
     ]
    }
   ],
   "source": [
    "def load_pdf_file(path):\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# Use raw string (r\"...\") to avoid escape errors\n",
    "path = r\"data\\21 Lessons for the 21st Century by Yuval Noah Harari.pdf\"\n",
    "\n",
    "extracted_data = load_pdf_file(path)\n",
    "print(len(extracted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa587b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting CSV normalization with unified argument types...\n",
      "Processing data\\1.csv...\n",
      "Processing data\\2.csv...\n",
      "Processing data\\3.csv...\n",
      "Processing data\\4.csv...\n",
      "Processing data\\5.csv...\n",
      "Processing data\\6.csv...\n",
      "Processing data\\7.csv...\n",
      "Processing data\\8.csv...\n",
      "Processing data\\9.csv...\n",
      "‚úÖ Processed 1123 rows\n",
      "üíæ Saved: normalized_debate_data_unified.csv\n",
      "üíæ Saved: rag_debate_documents_unified.csv\n",
      "\n",
      "üìä STATISTICS:\n",
      "Total pairs: 1123\n",
      "Unique topics: 25\n",
      "Topics: ['Internetaccess', 'Groundzeromosque', 'Militaryservice', 'Noflyzone', 'Securityprofiling', 'Solarenergy', 'Gasvehicles', 'Cellphones', 'Marijuanafree', 'Gaymarriage', 'Vegetarianism', 'Violentgames', 'Chinaonechildpolicy', 'Cocanarcotic', 'Childbeautycontests', 'Arminglibianrebels', 'Sobrietytest', 'Osamaphoto', 'Privatizingsocialsecurity', 'Tablet', 'Obesity', 'Abortion', 'Act1-TwelveAngryMan', 'Act2-TwelveAngryMan', 'Act3-TwelveAngryMan']\n",
      "Argument type distribution:\n",
      "argument_type\n",
      "attack     629\n",
      "support    344\n",
      "neutral    150\n",
      "Name: count, dtype: int64\n",
      "\n",
      "RAG documents created: 2237\n",
      "\n",
      "üëÄ PREVIEW OF UNIFIED DATA:\n",
      "              topic                                       premise_text  \\\n",
      "0    Internetaccess  The right to speak springs from the innate sen...   \n",
      "1  Groundzeromosque  Cordoba House is no act of tolerance, but of e...   \n",
      "2  Groundzeromosque  Cordoba House is a place of tolerance and inte...   \n",
      "3  Groundzeromosque  Victims' families view the imam's expressed pl...   \n",
      "4  Groundzeromosque  Name \"Cordoba House\" is a very direct historic...   \n",
      "5  Groundzeromosque  Name \"Cordoba House\" is about past period of t...   \n",
      "6   Militaryservice  Service helps connect and check military with ...   \n",
      "7   Militaryservice  Conscripts are never as good as professional s...   \n",
      "8   Militaryservice  Conscripts will not last as long nor be as com...   \n",
      "9   Militaryservice  Compulsory service brings in unqualified and u...   \n",
      "\n",
      "                                     hypothesis_text argument_type  \n",
      "0  Internet access is essential now; must be a ri...        attack  \n",
      "1  The intentions of Cordoba House (also referred...        attack  \n",
      "2  Cordoba House is no act of tolerance, but of e...        attack  \n",
      "3  The intentions of Cordoba House (also referred...        attack  \n",
      "4  The intentions of Cordoba House (also referred...        attack  \n",
      "5  Name \"Cordoba House\" is a very direct historic...        attack  \n",
      "6  National conscription is important to national...       support  \n",
      "7  National conscription is important to national...        attack  \n",
      "8  Conscripts are never as good as professional s...       support  \n",
      "9  Conscripts are never as good as professional s...       support  \n",
      "\n",
      "üîç MAPPING EXAMPLES:\n",
      "Entailment: 'NO' + Argument: '' ‚Üí Unified: 'attack'\n",
      "Entailment: 'NO' + Argument: '' ‚Üí Unified: 'attack'\n",
      "Entailment: 'NO' + Argument: '' ‚Üí Unified: 'attack'\n",
      "Entailment: 'NO' + Argument: '' ‚Üí Unified: 'attack'\n",
      "Entailment: 'NO' + Argument: '' ‚Üí Unified: 'attack'\n"
     ]
    }
   ],
   "source": [
    "# Quick Setup and Usage Example\n",
    "# Save this as normalize_csvs.py and run it\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def quick_normalize_csvs(data_folder=\"data\"):\n",
    "    \"\"\"Quick function to normalize your CSV files\"\"\"\n",
    "    \n",
    "    import glob\n",
    "    csv_files = glob.glob(os.path.join(data_folder, \"*.csv\"))\n",
    "    all_data = []\n",
    "    \n",
    "    for file_path in csv_files:\n",
    "        print(f\"Processing {file_path}...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            file_name = os.path.basename(file_path).replace('.csv', '')\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                # Extract data with error handling\n",
    "                normalized_row = {}\n",
    "                \n",
    "                # Basic info\n",
    "                normalized_row['file_id'] = file_name\n",
    "                normalized_row['pair_id'] = str(row.get('_id', ''))\n",
    "                normalized_row['topic'] = str(row.get('_topic', ''))\n",
    "                normalized_row['task'] = str(row.get('_task', 'ARG'))\n",
    "                \n",
    "                # Premise info (handle different formats)\n",
    "                premise_id = \"\"\n",
    "                premise_text = \"\"\n",
    "                \n",
    "                if 't/_id' in row.index:\n",
    "                    premise_id = str(row['t/_id']) if pd.notna(row['t/_id']) else \"\"\n",
    "                elif 't/0/_id' in row.index:\n",
    "                    premise_id = str(row['t/0/_id']) if pd.notna(row['t/0/_id']) else \"\"\n",
    "                \n",
    "                if 't/__text' in row.index:\n",
    "                    premise_text = str(row['t/__text']) if pd.notna(row['t/__text']) else \"\"\n",
    "                elif 't/0/__text' in row.index:\n",
    "                    premise_text = str(row['t/0/__text']) if pd.notna(row['t/0/__text']) else \"\"\n",
    "                \n",
    "                # Hypothesis info\n",
    "                hypothesis_id = str(row.get('h/_id', '')) if pd.notna(row.get('h/_id', '')) else \"\"\n",
    "                hypothesis_text = str(row.get('h/__text', '')) if pd.notna(row.get('h/__text', '')) else \"\"\n",
    "                \n",
    "                # COMBINE ENTAILMENT AND ARGUMENT INTO UNIFIED ARGUMENT_TYPE\n",
    "                # First get entailment value\n",
    "                entailment_raw = row.get('_entailment', '') or row.get('_ENTAILMENT', '')\n",
    "                \n",
    "                # Then get argument value\n",
    "                argument_raw = ''\n",
    "                if '_argument' in row.index and pd.notna(row['_argument']):\n",
    "                    argument_raw = str(row['_argument']).lower()\n",
    "                elif '_BAF' in row.index and pd.notna(row['_BAF']):\n",
    "                    argument_raw = str(row['_BAF']).lower()\n",
    "                \n",
    "                # Unified argument type mapping\n",
    "                if pd.isna(entailment_raw) and not argument_raw:\n",
    "                    argument_type = 'neutral'\n",
    "                else:\n",
    "                    # Map entailment to argument type\n",
    "                    if str(entailment_raw).upper() in ['YES', 'ENTAILMENT']:\n",
    "                        argument_type = 'support'\n",
    "                    elif str(entailment_raw).upper() in ['NO', 'CONTRADICTION']:\n",
    "                        argument_type = 'attack'\n",
    "                    else:\n",
    "                        # Use the original argument value if available\n",
    "                        argument_type = argument_raw if argument_raw else 'neutral'\n",
    "                \n",
    "                # Ensure we only have attack, support, or neutral\n",
    "                if argument_type not in ['attack', 'support', 'neutral']:\n",
    "                    argument_type = 'neutral'\n",
    "                \n",
    "                # Store normalized data\n",
    "                normalized_row.update({\n",
    "                    'premise_id': premise_id,\n",
    "                    'premise_text': premise_text.strip(),\n",
    "                    'hypothesis_id': hypothesis_id, \n",
    "                    'hypothesis_text': hypothesis_text.strip(),\n",
    "                    'argument_type': argument_type,  # Unified column\n",
    "                    'original_entailment': str(entailment_raw) if pd.notna(entailment_raw) else '',\n",
    "                    'original_argument': argument_raw,\n",
    "                    'complex_attack_type': str(row.get('_complex-attack', '')),\n",
    "                    'source_file': file_path\n",
    "                })\n",
    "                \n",
    "                all_data.append(normalized_row)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_normalized = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Clean empty rows\n",
    "    df_normalized = df_normalized[\n",
    "        (df_normalized['premise_text'] != '') | \n",
    "        (df_normalized['hypothesis_text'] != '')\n",
    "    ]\n",
    "    \n",
    "    # Add helper columns\n",
    "    df_normalized['unique_id'] = df_normalized['file_id'] + '_' + df_normalized['pair_id']\n",
    "    df_normalized['content_length'] = (df_normalized['premise_text'].str.len() + \n",
    "                                     df_normalized['hypothesis_text'].str.len())\n",
    "    \n",
    "    return df_normalized\n",
    "\n",
    "def create_rag_documents(df):\n",
    "    \"\"\"Convert to RAG-ready format\"\"\"\n",
    "    rag_docs = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Create combined context for better retrieval\n",
    "        combined_context = f\"TOPIC: {row['topic']} | ARGUMENT_TYPE: {row['argument_type']}\"\n",
    "        \n",
    "        if row['premise_text'].strip():\n",
    "            rag_docs.append({\n",
    "                'doc_id': f\"{row['unique_id']}_premise\",\n",
    "                'content': row['premise_text'], \n",
    "                'doc_type': 'premise',\n",
    "                'topic': row['topic'],\n",
    "                'argument_type': row['argument_type'],\n",
    "                'metadata': combined_context,\n",
    "                'full_context': f\"{combined_context} | PREMISE: {row['premise_text']}\"\n",
    "            })\n",
    "        \n",
    "        if row['hypothesis_text'].strip():\n",
    "            rag_docs.append({\n",
    "                'doc_id': f\"{row['unique_id']}_hypothesis\",\n",
    "                'content': row['hypothesis_text'],\n",
    "                'doc_type': 'hypothesis', \n",
    "                'topic': row['topic'],\n",
    "                'argument_type': row['argument_type'],\n",
    "                'metadata': combined_context,\n",
    "                'full_context': f\"{combined_context} | HYPOTHESIS: {row['hypothesis_text']}\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rag_docs)\n",
    "\n",
    "# Run the normalization\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting CSV normalization with unified argument types...\")\n",
    "    \n",
    "    # Normalize all CSVs in the data folder\n",
    "    normalized_df = quick_normalize_csvs(\"data\")  # Change \"data\" to your folder path\n",
    "    \n",
    "    print(f\"‚úÖ Processed {len(normalized_df)} rows\")\n",
    "    \n",
    "    # Save normalized data\n",
    "    normalized_df.to_csv(\"normalized_debate_data_unified.csv\", index=False)\n",
    "    print(\"üíæ Saved: normalized_debate_data_unified.csv\")\n",
    "    \n",
    "    # Create RAG format\n",
    "    rag_df = create_rag_documents(normalized_df)\n",
    "    rag_df.to_csv(\"rag_debate_documents_unified.csv\", index=False) \n",
    "    print(\"üíæ Saved: rag_debate_documents_unified.csv\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nüìä STATISTICS:\")\n",
    "    print(f\"Total pairs: {len(normalized_df)}\")\n",
    "    print(f\"Unique topics: {normalized_df['topic'].nunique()}\")\n",
    "    print(f\"Topics: {list(normalized_df['topic'].unique())}\")\n",
    "    print(f\"Argument type distribution:\")\n",
    "    print(normalized_df['argument_type'].value_counts())\n",
    "    \n",
    "    print(f\"\\nRAG documents created: {len(rag_df)}\")\n",
    "    \n",
    "    # Show preview\n",
    "    print(f\"\\nüëÄ PREVIEW OF UNIFIED DATA:\")\n",
    "    print(normalized_df[['topic', 'premise_text', 'hypothesis_text', 'argument_type']].head(10))\n",
    "    \n",
    "    # Show mapping examples\n",
    "    print(f\"\\nüîç MAPPING EXAMPLES:\")\n",
    "    sample = normalized_df[['original_entailment', 'original_argument', 'argument_type']].head(5)\n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"Entailment: '{row['original_entailment']}' + Argument: '{row['original_argument']}' ‚Üí Unified: '{row['argument_type']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42296c16",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Now check again\u001b[39;00m\n\u001b[0;32m      7\u001b[0m PINECONE_API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPINECONE_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpinecone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pinecone\n\u001b[0;32m      9\u001b[0m pinecone_api_key\u001b[38;5;241m=\u001b[39mPINECONE_API_KEY\n\u001b[0;32m     10\u001b[0m pc\u001b[38;5;241m=\u001b[39mPinecone(api_key\u001b[38;5;241m=\u001b[39mpinecone_api_key)\n",
      "File \u001b[1;32mc:\\Users\\nihar\\.conda\\envs\\debatebot\\lib\\site-packages\\pinecone\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. include:: ../README.md\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mException\u001b[0m: The official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK."
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Reload fresh from .env file\n",
    "load_dotenv(override=True)  # ‚Üê Add override=True\n",
    "\n",
    "# Now check again\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "from pinecone import Pinecone\n",
    "pinecone_api_key=PINECONE_API_KEY\n",
    "pc=Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb52f38",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpinecone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pinecone, ServerlessSpec\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load env vars\u001b[39;00m\n\u001b[0;32m      9\u001b[0m load_dotenv(override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\nihar\\.conda\\envs\\debatebot\\lib\\site-packages\\pinecone\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. include:: ../README.md\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mException\u001b[0m: The official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK."
     ]
    }
   ],
   "source": [
    "# setup_pinecone.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Load env vars\n",
    "load_dotenv(override=True)\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Load your RAG debate dataset\n",
    "df = pd.read_csv(\"rag_debate_documents_unified.csv\")\n",
    "\n",
    "# Embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"debate-knowledge-base\"\n",
    "\n",
    "# Create index if not exists\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # embedding dim for MiniLM\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "# Connect to index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Prepare and upsert embeddings\n",
    "embeddings = model.encode(df[\"content\"].tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "vectors = []\n",
    "for i, (emb, meta) in enumerate(zip(embeddings, df.to_dict(orient=\"records\"))):\n",
    "    vectors.append({\n",
    "        \"id\": str(i),       # unique ID\n",
    "        \"values\": emb.tolist(),\n",
    "        \"metadata\": meta\n",
    "    })\n",
    "\n",
    "# Batch upload\n",
    "batch_size = 100\n",
    "for i in range(0, len(vectors), batch_size):\n",
    "    index.upsert(vectors[i:i+batch_size])\n",
    "\n",
    "print(f\"‚úÖ Uploaded {len(vectors)} debate documents to Pinecone\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbc581b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:16<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded 2237 debate documents to Pinecone\n"
     ]
    }
   ],
   "source": [
    "# setup_pinecone.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv(override=True)\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"rag_debate_documents_unified.csv\")\n",
    "\n",
    "# Embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"debate-knowledge-base\"\n",
    "\n",
    "# Create index if it does not exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # embedding size for MiniLM\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Encode embeddings\n",
    "embeddings = model.encode(df[\"content\"].tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Format vectors with metadata\n",
    "vectors = []\n",
    "for i, (emb, meta) in enumerate(zip(embeddings, df.to_dict(orient=\"records\"))):\n",
    "    vectors.append({\n",
    "        \"id\": str(i),\n",
    "        \"values\": emb.tolist(),\n",
    "        \"metadata\": meta\n",
    "    })\n",
    "\n",
    "# Batch upload\n",
    "batch_size = 100\n",
    "for i in range(0, len(vectors), batch_size):\n",
    "    index.upsert(vectors[i:i+batch_size])\n",
    "\n",
    "print(f\"‚úÖ Uploaded {len(vectors)} debate documents to Pinecone\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bada3682",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpinecone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pinecone\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load env\u001b[39;00m\n\u001b[0;32m      8\u001b[0m load_dotenv(override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\nihar\\.conda\\envs\\debatebot\\lib\\site-packages\\pinecone\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. include:: ../README.md\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mException\u001b[0m: The official Pinecone python package has been renamed from `pinecone-client` to `pinecone`. Please remove `pinecone-client` from your project dependencies and add `pinecone` instead. See the README at https://github.com/pinecone-io/pinecone-python-client for more information on using the python SDK."
     ]
    }
   ],
   "source": [
    "# debate_bot_pinecone.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Load env\n",
    "load_dotenv(override=True)\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Init Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(\"debate-knowledge-base\")\n",
    "\n",
    "# Embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def debate_response(user_input, user_side=\"support\"):\n",
    "    \"\"\"\n",
    "    user_side = 'support' or 'attack'\n",
    "    The bot retrieves counter-arguments (opposite side).\n",
    "    \"\"\"\n",
    "    user_emb = model.encode([user_input], convert_to_numpy=True)[0]\n",
    "\n",
    "    results = index.query(\n",
    "        vector=user_emb.tolist(),\n",
    "        top_k=5,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    counter_args = [\n",
    "        match[\"metadata\"] for match in results[\"matches\"]\n",
    "        if match[\"metadata\"][\"argument_type\"] != user_side\n",
    "    ]\n",
    "    return counter_args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_statement = \"I believe internet access should be a human right.\"\n",
    "    counter = debate_response(user_statement, user_side=\"support\")\n",
    "\n",
    "    print(\"üî• Counter Arguments Retrieved:\")\n",
    "    for arg in counter:\n",
    "        print(f\"- {arg['doc_type'].upper()} | {arg['topic']}: {arg['content'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# def debate_session(topic, chatModel, retriever, prompt, rounds=6):\n",
    "#     \"\"\"\n",
    "#     Conducts a turn-based debate for a given topic between the human (you)\n",
    "#     and the debate assistant (AI).\n",
    "#     Each side speaks alternately.\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"\\nüé§ Debate Topic: {topic}\")\n",
    "#     print(\"=\" * 70)\n",
    "\n",
    "#     # Create retrieval + QA chain\n",
    "#     question_answer_chain = create_stuff_documents_chain(chatModel, prompt)\n",
    "#     rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "#     user_turn = f\"I believe {topic} because...\"\n",
    "    \n",
    "#     for i in range(rounds):\n",
    "#         print(f\"\\nüßç‚Äç‚ôÇÔ∏è You: {user_turn}\")\n",
    "        \n",
    "#         # Model‚Äôs turn ‚Äî responds using context\n",
    "#         response = rag_chain.invoke({\"input\": user_turn})\n",
    "#         model_reply = response[\"answer\"]\n",
    "\n",
    "#         print(f\"\\nü§ñ DebateBot: {model_reply}\")\n",
    "\n",
    "#         # Add a small pause for readability (simulate real-time debate)\n",
    "#         time.sleep(2)\n",
    "\n",
    "#         # Optional: Auto-generate a counter-argument for realism\n",
    "#         if i < rounds - 1:\n",
    "#             user_turn = f\"However, I would argue that {topic} might also have opposing concerns, such as {model_reply.split('.')[0].lower()}...\"\n",
    "    \n",
    "#     print(\"\\nüèÅ Debate concluded! Great discussion üëè\")\n",
    "\n",
    "# # Example usage:\n",
    "# debate_session(\n",
    "#     topic=\"Artificial intelligence should be regulated by governments.\",\n",
    "#     chatModel=chatModel,\n",
    "#     retriever=retreiver,\n",
    "#     prompt=prompt,\n",
    "#     rounds=6  # about 1 minute of debate\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debatebot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
